{
  "models": [
    {
      "provider_name": "OpenAI",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "gpt-4o",
      "version": "2024-05-13",
      "model_task": "text-generation",
      "model_family": "gpt-4",
      "replicas": 25,
      "max_concurrency": 120,
      "ideal_concurrency": 100,
      "accel_type": "A100",
      "gpus_per_replica": 4,
      "memory_gb": 80,
      "max_rps": 60,
      "endpoint": "https://api.openai.com/v1/chat/completions",
      "is_open_source": false,
      "model_variant": "gpt-4o_128k_20240513",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 1200000,
      "avg_tokens_per_request": 2500,
      "avg_latency_seconds": 0.180,
      "snapshot_date": "2025-02-01"
    },
    {
      "provider_name": "Anthropic",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "claude-3.5-sonnet",
      "version": "20241022",
      "model_task": "text-generation",
      "model_family": "claude-3",
      "replicas": 20,
      "max_concurrency": 150,
      "ideal_concurrency": 120,
      "accel_type": "H100",
      "gpus_per_replica": 2,
      "memory_gb": 80,
      "max_rps": 55,
      "endpoint": "https://api.anthropic.com/v1/messages",
      "is_open_source": false,
      "model_variant": "claude-3.5-sonnet_200k_20241022",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 1800000,
      "avg_tokens_per_request": 2800,
      "avg_latency_seconds": 0.190,
      "snapshot_date": "2025-02-01"
    },
    {
      "provider_name": "OpenAI",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "gpt-4-turbo",
      "version": "2024-04-09",
      "model_task": "text-generation",
      "model_family": "gpt-4",
      "replicas": 18,
      "max_concurrency": 100,
      "ideal_concurrency": 80,
      "accel_type": "A100",
      "gpus_per_replica": 4,
      "memory_gb": 80,
      "max_rps": 50,
      "endpoint": "https://api.openai.com/v1/chat/completions",
      "is_open_source": false,
      "model_variant": "gpt-4-turbo_128k_20240409",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 1100000,
      "avg_tokens_per_request": 2400,
      "avg_latency_seconds": 0.210,
      "snapshot_date": "2025-02-01"
    },
    {
      "provider_name": "Meta",
      "provider_type": "open-source",
      "platform_name": "model_serving_platform",
      "model_name": "llama-3.1",
      "version": "3.1",
      "model_task": "text-generation",
      "model_family": "llama",
      "replicas": 22,
      "max_concurrency": 80,
      "ideal_concurrency": 60,
      "accel_type": "A100",
      "gpus_per_replica": 2,
      "memory_gb": 140,
      "max_rps": 45,
      "endpoint": "https://api.together.xyz/v1/chat/completions",
      "is_open_source": true,
      "model_variant": "llama-3.1_70b_20240723",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 750000,
      "avg_tokens_per_request": 2200,
      "avg_latency_seconds": 0.240,
      "snapshot_date": "2025-01-15"
    },
    {
      "provider_name": "Google",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "gemini-1.5-pro",
      "version": "1.5",
      "model_task": "text-generation",
      "model_family": "gemini",
      "replicas": 16,
      "max_concurrency": 100,
      "ideal_concurrency": 80,
      "accel_type": "TPU-v5",
      "gpus_per_replica": 4,
      "memory_gb": 100,
      "max_rps": 48,
      "endpoint": "https://generativelanguage.googleapis.com/v1/models",
      "is_open_source": false,
      "model_variant": "gemini-1.5-pro_1m_20240214",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 1300000,
      "avg_tokens_per_request": 2600,
      "avg_latency_seconds": 0.200,
      "snapshot_date": "2025-02-01"
    },
    {
      "provider_name": "OpenAI",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "gpt-3.5-turbo",
      "version": "0125",
      "model_task": "text-generation",
      "model_family": "gpt-3.5",
      "replicas": 30,
      "max_concurrency": 200,
      "ideal_concurrency": 160,
      "accel_type": "A10",
      "gpus_per_replica": 1,
      "memory_gb": 24,
      "max_rps": 100,
      "endpoint": "https://api.openai.com/v1/chat/completions",
      "is_open_source": false,
      "model_variant": "gpt-3.5-turbo_16k_20250125",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 2500000,
      "avg_tokens_per_request": 1800,
      "avg_latency_seconds": 0.110,
      "snapshot_date": "2025-02-01"
    },
    {
      "provider_name": "Mistral AI",
      "provider_type": "open-source",
      "platform_name": "model_serving_platform",
      "model_name": "mixtral-8x7b",
      "version": "v0.1",
      "model_task": "text-generation",
      "model_family": "mixtral",
      "replicas": 15,
      "max_concurrency": 70,
      "ideal_concurrency": 50,
      "accel_type": "A100",
      "gpus_per_replica": 2,
      "memory_gb": 90,
      "max_rps": 40,
      "endpoint": "https://api.together.xyz/v1/chat/completions",
      "is_open_source": true,
      "model_variant": "mixtral-8x7b_32k_20231211",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 850000,
      "avg_tokens_per_request": 2000,
      "avg_latency_seconds": 0.220,
      "snapshot_date": "2025-01-10"
    },
    {
      "provider_name": "Anthropic",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "claude-3-opus",
      "version": "20240229",
      "model_task": "text-generation",
      "model_family": "claude-3",
      "replicas": 12,
      "max_concurrency": 60,
      "ideal_concurrency": 50,
      "accel_type": "H100",
      "gpus_per_replica": 4,
      "memory_gb": 160,
      "max_rps": 35,
      "endpoint": "https://api.anthropic.com/v1/messages",
      "is_open_source": false,
      "model_variant": "claude-3-opus_200k_20240229",
      "traffic_type": "continent",
      "tokens_per_second_per_instance": 900000,
      "avg_tokens_per_request": 3200,
      "avg_latency_seconds": 0.350,
      "snapshot_date": "2025-02-01"
    },
    {
      "provider_name": "Cohere",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "command-r-plus",
      "version": "2024-04",
      "model_task": "text-generation",
      "model_family": "command",
      "replicas": 14,
      "max_concurrency": 80,
      "ideal_concurrency": 65,
      "accel_type": "A100",
      "gpus_per_replica": 2,
      "memory_gb": 80,
      "max_rps": 42,
      "endpoint": "https://api.cohere.ai/v1/chat",
      "is_open_source": false,
      "model_variant": "command-r-plus_128k_20240404",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 1000000,
      "avg_tokens_per_request": 2300,
      "avg_latency_seconds": 0.230,
      "snapshot_date": "2025-01-20"
    },
    {
      "provider_name": "Meta",
      "provider_type": "open-source",
      "platform_name": "model_serving_platform",
      "model_name": "llama-3.1",
      "version": "3.1",
      "model_task": "text-generation",
      "model_family": "llama",
      "replicas": 8,
      "max_concurrency": 40,
      "ideal_concurrency": 30,
      "accel_type": "A100",
      "gpus_per_replica": 8,
      "memory_gb": 640,
      "max_rps": 25,
      "endpoint": "https://api.together.xyz/v1/chat/completions",
      "is_open_source": true,
      "model_variant": "llama-3.1_405b_20240723",
      "traffic_type": "region",
      "tokens_per_second_per_instance": 400000,
      "avg_tokens_per_request": 3000,
      "avg_latency_seconds": 0.450,
      "snapshot_date": "2025-01-15"
    },
    {
      "provider_name": "Anthropic",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "claude-3-sonnet",
      "version": "20240229",
      "model_task": "text-generation",
      "model_family": "claude-3",
      "replicas": 18,
      "max_concurrency": 100,
      "ideal_concurrency": 80,
      "accel_type": "H100",
      "gpus_per_replica": 2,
      "memory_gb": 80,
      "max_rps": 50,
      "endpoint": "https://api.anthropic.com/v1/messages",
      "is_open_source": false,
      "model_variant": "claude-3-sonnet_200k_20240229",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 1400000,
      "avg_tokens_per_request": 2500,
      "avg_latency_seconds": 0.190,
      "snapshot_date": "2025-02-01"
    },
    {
      "provider_name": "Google",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "gemini-1.5-flash",
      "version": "1.5",
      "model_task": "text-generation",
      "model_family": "gemini",
      "replicas": 24,
      "max_concurrency": 150,
      "ideal_concurrency": 120,
      "accel_type": "TPU-v5",
      "gpus_per_replica": 2,
      "memory_gb": 48,
      "max_rps": 80,
      "endpoint": "https://generativelanguage.googleapis.com/v1/models",
      "is_open_source": false,
      "model_variant": "gemini-1.5-flash_1m_20240214",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 2200000,
      "avg_tokens_per_request": 1900,
      "avg_latency_seconds": 0.120,
      "snapshot_date": "2025-02-01"
    },
    {
      "provider_name": "Mistral AI",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "mistral-large",
      "version": "2",
      "model_task": "text-generation",
      "model_family": "mistral",
      "replicas": 10,
      "max_concurrency": 60,
      "ideal_concurrency": 45,
      "accel_type": "A100",
      "gpus_per_replica": 4,
      "memory_gb": 160,
      "max_rps": 38,
      "endpoint": "https://api.mistral.ai/v1/chat/completions",
      "is_open_source": false,
      "model_variant": "mistral-large_128k_20240724",
      "traffic_type": "territory",
      "tokens_per_second_per_instance": 950000,
      "avg_tokens_per_request": 2700,
      "avg_latency_seconds": 0.280,
      "snapshot_date": "2025-01-25"
    },
    {
      "provider_name": "Meta",
      "provider_type": "open-source",
      "platform_name": "model_serving_platform",
      "model_name": "llama-3",
      "version": "3.0",
      "model_task": "text-generation",
      "model_family": "llama",
      "replicas": 20,
      "max_concurrency": 100,
      "ideal_concurrency": 80,
      "accel_type": "A10",
      "gpus_per_replica": 1,
      "memory_gb": 48,
      "max_rps": 55,
      "endpoint": "https://api.together.xyz/v1/chat/completions",
      "is_open_source": true,
      "model_variant": "llama-3_8b_20240418",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 1800000,
      "avg_tokens_per_request": 1600,
      "avg_latency_seconds": 0.140,
      "snapshot_date": "2024-12-15"
    },
    {
      "provider_name": "OpenAI",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "gpt-4",
      "version": "0613",
      "model_task": "text-generation",
      "model_family": "gpt-4",
      "replicas": 15,
      "max_concurrency": 80,
      "ideal_concurrency": 65,
      "accel_type": "A100",
      "gpus_per_replica": 4,
      "memory_gb": 80,
      "max_rps": 42,
      "endpoint": "https://api.openai.com/v1/chat/completions",
      "is_open_source": false,
      "model_variant": "gpt-4_8k_20230613",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 1000000,
      "avg_tokens_per_request": 2500,
      "avg_latency_seconds": 0.250,
      "snapshot_date": "2025-02-01"
    },
    {
      "provider_name": "Mistral AI",
      "provider_type": "open-source",
      "platform_name": "model_serving_platform",
      "model_name": "mistral",
      "version": "7b-v0.3",
      "model_task": "text-generation",
      "model_family": "mistral",
      "replicas": 25,
      "max_concurrency": 120,
      "ideal_concurrency": 100,
      "accel_type": "A10",
      "gpus_per_replica": 1,
      "memory_gb": 16,
      "max_rps": 75,
      "endpoint": "https://api.together.xyz/v1/chat/completions",
      "is_open_source": true,
      "model_variant": "mistral_7b_32k_20240522",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 2800000,
      "avg_tokens_per_request": 1500,
      "avg_latency_seconds": 0.095,
      "snapshot_date": "2024-11-20"
    },
    {
      "provider_name": "Anthropic",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "claude-3-haiku",
      "version": "20240307",
      "model_task": "text-generation",
      "model_family": "claude-3",
      "replicas": 28,
      "max_concurrency": 180,
      "ideal_concurrency": 150,
      "accel_type": "A10",
      "gpus_per_replica": 1,
      "memory_gb": 40,
      "max_rps": 90,
      "endpoint": "https://api.anthropic.com/v1/messages",
      "is_open_source": false,
      "model_variant": "claude-3-haiku_200k_20240307",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 2600000,
      "avg_tokens_per_request": 1700,
      "avg_latency_seconds": 0.100,
      "snapshot_date": "2025-02-01"
    },
    {
      "provider_name": "Google",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "gemini-1.0-pro",
      "version": "1.0",
      "model_task": "text-generation",
      "model_family": "gemini",
      "replicas": 16,
      "max_concurrency": 90,
      "ideal_concurrency": 70,
      "accel_type": "TPU-v4",
      "gpus_per_replica": 2,
      "memory_gb": 60,
      "max_rps": 48,
      "endpoint": "https://generativelanguage.googleapis.com/v1/models",
      "is_open_source": false,
      "model_variant": "gemini-1.0-pro_32k_20231206",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 1500000,
      "avg_tokens_per_request": 2100,
      "avg_latency_seconds": 0.170,
      "snapshot_date": "2024-12-01"
    },
    {
      "provider_name": "Cohere",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "command",
      "version": "2024-03",
      "model_task": "text-generation",
      "model_family": "command",
      "replicas": 12,
      "max_concurrency": 70,
      "ideal_concurrency": 55,
      "accel_type": "A100",
      "gpus_per_replica": 2,
      "memory_gb": 60,
      "max_rps": 40,
      "endpoint": "https://api.cohere.ai/v1/chat",
      "is_open_source": false,
      "model_variant": "command_128k_20240301",
      "traffic_type": "all",
      "tokens_per_second_per_instance": 1100000,
      "avg_tokens_per_request": 2000,
      "avg_latency_seconds": 0.210,
      "snapshot_date": "2025-01-15"
    },
    {
      "provider_name": "AI21 Labs",
      "provider_type": "proprietary",
      "platform_name": "model_serving_platform",
      "model_name": "jamba-1.5-large",
      "version": "1.5",
      "model_task": "text-generation",
      "model_family": "jamba",
      "replicas": 10,
      "max_concurrency": 50,
      "ideal_concurrency": 40,
      "accel_type": "A100",
      "gpus_per_replica": 4,
      "memory_gb": 120,
      "max_rps": 35,
      "endpoint": "https://api.ai21.com/studio/v1/chat/completions",
      "is_open_source": false,
      "model_variant": "jamba-1.5-large_256k_20240815",
      "traffic_type": "territory",
      "tokens_per_second_per_instance": 950000,
      "avg_tokens_per_request": 2400,
      "avg_latency_seconds": 0.260,
      "snapshot_date": "2025-01-10"
    }
  ],
  "metadata": {
    "source": "model_serving_platform",
    "extracted_at": "2026-02-13T19:30:00Z",
    "total_models": 20,
    "open_source_count": 6,
    "proprietary_count": 14,
    "data_version": "1.0",
    "model_variant_format": "modelname_contextsize_releasedate (all lowercase)",
    "data_classification": {
      "publicly_available": [
        "provider_name",
        "provider_type",
        "model_name",
        "version",
        "model_task",
        "model_family",
        "endpoint",
        "is_open_source",
        "model_variant (model name, context window size, release date are publicly documented)"
      ],
      "synthesized_from_public_benchmarks": [
        "accel_type (common GPU types for these model sizes)",
        "gpus_per_replica (estimated from model size and typical deployments)",
        "memory_gb (estimated from model parameters and precision)",
        "tokens_per_second_per_instance (derived from public benchmarks)",
        "avg_tokens_per_request (typical usage patterns)",
        "avg_latency_seconds (based on public API performance reports)"
      ],
      "synthesized_deployment_estimates": [
        "replicas (estimated production scale)",
        "max_concurrency (estimated from typical API limits)",
        "ideal_concurrency (estimated optimal throughput)",
        "max_rps (estimated rate limits)",
        "traffic_type (inferred deployment scope)",
        "platform_name (standardized value)",
        "snapshot_date (date of data compilation)"
      ]
    },
    "notes": [
      "Model names, versions, context windows, and release dates are from official provider documentation",
      "Performance metrics synthesized from public benchmarks and API documentation",
      "Deployment configurations (replicas, concurrency) are estimates based on typical production patterns",
      "Actual production values will vary by specific deployment and workload"
    ]
  }
}
