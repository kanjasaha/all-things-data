{
  "models": [
    {
      "model_resource_name": "openai/gpt-4o",
      "model_display_name": "gpt-4o",
      "model_version": "2024-05-13",
      "publisher_name": "OpenAI",
      "model_family": "gpt-4",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "global",
      "endpoint": "https://api.openai.com/v1/chat/completions",
      "accelerator_type": "A100",
      "accelerators_per_replica": 4,
      "memory_gb": 80,
      "replicas": 25,
      "max_concurrency": 120,
      "ideal_concurrency": 100,
      "max_rps": 60,
      "tokens_per_second": 1200000,
      "avg_tokens_per_request": 2500,
      "avg_latency_seconds": 0.18,
      "model_variant": "gpt-4o_128k_20240513",
      "snapshot_date": "2025-02-01"
    },
    {
      "model_resource_name": "anthropic/claude-3.5-sonnet",
      "model_display_name": "claude-3.5-sonnet",
      "model_version": "20241022",
      "publisher_name": "Anthropic",
      "model_family": "claude-3",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "global",
      "endpoint": "https://api.anthropic.com/v1/messages",
      "accelerator_type": "H100",
      "accelerators_per_replica": 2,
      "memory_gb": 80,
      "replicas": 20,
      "max_concurrency": 150,
      "ideal_concurrency": 120,
      "max_rps": 55,
      "tokens_per_second": 1800000,
      "avg_tokens_per_request": 2800,
      "avg_latency_seconds": 0.19,
      "model_variant": "claude-3.5-sonnet_200k_20241022",
      "snapshot_date": "2025-02-01"
    },
    {
      "model_resource_name": "openai/gpt-4-turbo",
      "model_display_name": "gpt-4-turbo",
      "model_version": "2024-04-09",
      "publisher_name": "OpenAI",
      "model_family": "gpt-4",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "global",
      "endpoint": "https://api.openai.com/v1/chat/completions",
      "accelerator_type": "A100",
      "accelerators_per_replica": 4,
      "memory_gb": 80,
      "replicas": 18,
      "max_concurrency": 100,
      "ideal_concurrency": 80,
      "max_rps": 50,
      "tokens_per_second": 1100000,
      "avg_tokens_per_request": 2400,
      "avg_latency_seconds": 0.21,
      "model_variant": "gpt-4-turbo_128k_20240409",
      "snapshot_date": "2025-02-01"
    },
    {
      "model_resource_name": "meta/llama-3.1-70b",
      "model_display_name": "llama-3.1-70b",
      "model_version": "3.1",
      "publisher_name": "Meta",
      "model_family": "llama",
      "model_task": "text-generation",
      "is_open_source": true,
      "inference_scope": "global",
      "endpoint": "https://api.together.xyz/v1/chat/completions",
      "accelerator_type": "A100",
      "accelerators_per_replica": 2,
      "memory_gb": 140,
      "replicas": 22,
      "max_concurrency": 80,
      "ideal_concurrency": 60,
      "max_rps": 45,
      "tokens_per_second": 750000,
      "avg_tokens_per_request": 2200,
      "avg_latency_seconds": 0.24,
      "model_variant": "llama-3.1_70b_20240723",
      "snapshot_date": "2025-01-15"
    },
    {
      "model_resource_name": "google/gemini-1.5-pro",
      "model_display_name": "gemini-1.5-pro",
      "model_version": "1.5",
      "publisher_name": "Google",
      "model_family": "gemini",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "global",
      "endpoint": "https://generativelanguage.googleapis.com/v1/models",
      "accelerator_type": "TPU-v5",
      "accelerators_per_replica": 4,
      "memory_gb": 100,
      "replicas": 16,
      "max_concurrency": 100,
      "ideal_concurrency": 80,
      "max_rps": 48,
      "tokens_per_second": 1300000,
      "avg_tokens_per_request": 2600,
      "avg_latency_seconds": 0.2,
      "model_variant": "gemini-1.5-pro_1m_20240214",
      "snapshot_date": "2025-02-01"
    },
    {
      "model_resource_name": "openai/gpt-3.5-turbo",
      "model_display_name": "gpt-3.5-turbo",
      "model_version": "0125",
      "publisher_name": "OpenAI",
      "model_family": "gpt-3.5",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "global",
      "endpoint": "https://api.openai.com/v1/chat/completions",
      "accelerator_type": "A10",
      "accelerators_per_replica": 1,
      "memory_gb": 24,
      "replicas": 30,
      "max_concurrency": 200,
      "ideal_concurrency": 160,
      "max_rps": 100,
      "tokens_per_second": 2500000,
      "avg_tokens_per_request": 1800,
      "avg_latency_seconds": 0.11,
      "model_variant": "gpt-3.5-turbo_16k_20250125",
      "snapshot_date": "2025-02-01"
    },
    {
      "model_resource_name": "mistral/mixtral-8x7b",
      "model_display_name": "mixtral-8x7b",
      "model_version": "v0.1",
      "publisher_name": "Mistral AI",
      "model_family": "mixtral",
      "model_task": "text-generation",
      "is_open_source": true,
      "inference_scope": "global",
      "endpoint": "https://api.together.xyz/v1/chat/completions",
      "accelerator_type": "A100",
      "accelerators_per_replica": 2,
      "memory_gb": 90,
      "replicas": 15,
      "max_concurrency": 70,
      "ideal_concurrency": 50,
      "max_rps": 40,
      "tokens_per_second": 850000,
      "avg_tokens_per_request": 2000,
      "avg_latency_seconds": 0.22,
      "model_variant": "mixtral-8x7b_32k_20231211",
      "snapshot_date": "2025-01-10"
    },
    {
      "model_resource_name": "anthropic/claude-3-opus",
      "model_display_name": "claude-3-opus",
      "model_version": "20240229",
      "publisher_name": "Anthropic",
      "model_family": "claude-3",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "multi-region",
      "endpoint": "https://api.anthropic.com/v1/messages",
      "accelerator_type": "H100",
      "accelerators_per_replica": 4,
      "memory_gb": 160,
      "replicas": 12,
      "max_concurrency": 60,
      "ideal_concurrency": 50,
      "max_rps": 35,
      "tokens_per_second": 900000,
      "avg_tokens_per_request": 3200,
      "avg_latency_seconds": 0.35,
      "model_variant": "claude-3-opus_200k_20240229",
      "snapshot_date": "2025-02-01"
    },
    {
      "model_resource_name": "cohere/command-r-plus",
      "model_display_name": "command-r-plus",
      "model_version": "2024-04",
      "publisher_name": "Cohere",
      "model_family": "command",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "global",
      "endpoint": "https://api.cohere.ai/v1/chat",
      "accelerator_type": "A100",
      "accelerators_per_replica": 2,
      "memory_gb": 80,
      "replicas": 14,
      "max_concurrency": 80,
      "ideal_concurrency": 65,
      "max_rps": 42,
      "tokens_per_second": 1000000,
      "avg_tokens_per_request": 2300,
      "avg_latency_seconds": 0.23,
      "model_variant": "command-r-plus_128k_20240404",
      "snapshot_date": "2025-01-20"
    },
    {
      "model_resource_name": "meta/llama-3.1-405b",
      "model_display_name": "llama-3.1-405b",
      "model_version": "3.1",
      "publisher_name": "Meta",
      "model_family": "llama",
      "model_task": "text-generation",
      "is_open_source": true,
      "inference_scope": "regional",
      "endpoint": "https://api.together.xyz/v1/chat/completions",
      "accelerator_type": "A100",
      "accelerators_per_replica": 8,
      "memory_gb": 640,
      "replicas": 8,
      "max_concurrency": 40,
      "ideal_concurrency": 30,
      "max_rps": 25,
      "tokens_per_second": 400000,
      "avg_tokens_per_request": 3000,
      "avg_latency_seconds": 0.45,
      "model_variant": "llama-3.1_405b_20240723",
      "snapshot_date": "2025-01-15"
    },
    {
      "model_resource_name": "anthropic/claude-3-sonnet",
      "model_display_name": "claude-3-sonnet",
      "model_version": "20240229",
      "publisher_name": "Anthropic",
      "model_family": "claude-3",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "global",
      "endpoint": "https://api.anthropic.com/v1/messages",
      "accelerator_type": "H100",
      "accelerators_per_replica": 2,
      "memory_gb": 80,
      "replicas": 18,
      "max_concurrency": 100,
      "ideal_concurrency": 80,
      "max_rps": 50,
      "tokens_per_second": 1400000,
      "avg_tokens_per_request": 2500,
      "avg_latency_seconds": 0.19,
      "model_variant": "claude-3-sonnet_200k_20240229",
      "snapshot_date": "2025-02-01"
    },
    {
      "model_resource_name": "google/gemini-1.5-flash",
      "model_display_name": "gemini-1.5-flash",
      "model_version": "1.5",
      "publisher_name": "Google",
      "model_family": "gemini",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "global",
      "endpoint": "https://generativelanguage.googleapis.com/v1/models",
      "accelerator_type": "TPU-v5",
      "accelerators_per_replica": 2,
      "memory_gb": 48,
      "replicas": 24,
      "max_concurrency": 150,
      "ideal_concurrency": 120,
      "max_rps": 80,
      "tokens_per_second": 2200000,
      "avg_tokens_per_request": 1900,
      "avg_latency_seconds": 0.12,
      "model_variant": "gemini-1.5-flash_1m_20240214",
      "snapshot_date": "2025-02-01"
    },
    {
      "model_resource_name": "mistral/mistral-large",
      "model_display_name": "mistral-large",
      "model_version": "2",
      "publisher_name": "Mistral AI",
      "model_family": "mistral",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "multi-region",
      "endpoint": "https://api.mistral.ai/v1/chat/completions",
      "accelerator_type": "A100",
      "accelerators_per_replica": 4,
      "memory_gb": 160,
      "replicas": 10,
      "max_concurrency": 60,
      "ideal_concurrency": 45,
      "max_rps": 38,
      "tokens_per_second": 950000,
      "avg_tokens_per_request": 2700,
      "avg_latency_seconds": 0.28,
      "model_variant": "mistral-large_128k_20240724",
      "snapshot_date": "2025-01-25"
    },
    {
      "model_resource_name": "meta/llama-3_8b",
      "model_display_name": "llama-3_8b",
      "model_version": "3.0",
      "publisher_name": "Meta",
      "model_family": "llama",
      "model_task": "text-generation",
      "is_open_source": true,
      "inference_scope": "global",
      "endpoint": "https://api.together.xyz/v1/chat/completions",
      "accelerator_type": "A10",
      "accelerators_per_replica": 1,
      "memory_gb": 48,
      "replicas": 20,
      "max_concurrency": 100,
      "ideal_concurrency": 80,
      "max_rps": 55,
      "tokens_per_second": 1800000,
      "avg_tokens_per_request": 1600,
      "avg_latency_seconds": 0.14,
      "model_variant": "llama-3_8b_20240418",
      "snapshot_date": "2024-12-15"
    },
    {
      "model_resource_name": "openai/gpt-4",
      "model_display_name": "gpt-4",
      "model_version": "0613",
      "publisher_name": "OpenAI",
      "model_family": "gpt-4",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "global",
      "endpoint": "https://api.openai.com/v1/chat/completions",
      "accelerator_type": "A100",
      "accelerators_per_replica": 4,
      "memory_gb": 80,
      "replicas": 15,
      "max_concurrency": 80,
      "ideal_concurrency": 65,
      "max_rps": 42,
      "tokens_per_second": 1000000,
      "avg_tokens_per_request": 2500,
      "avg_latency_seconds": 0.25,
      "model_variant": "gpt-4_8k_20230613",
      "snapshot_date": "2025-02-01"
    },
    {
      "model_resource_name": "mistral/mistral-7b",
      "model_display_name": "mistral-7b",
      "model_version": "7b-v0.3",
      "publisher_name": "Mistral AI",
      "model_family": "mistral",
      "model_task": "text-generation",
      "is_open_source": true,
      "inference_scope": "global",
      "endpoint": "https://api.together.xyz/v1/chat/completions",
      "accelerator_type": "A10",
      "accelerators_per_replica": 1,
      "memory_gb": 16,
      "replicas": 25,
      "max_concurrency": 120,
      "ideal_concurrency": 100,
      "max_rps": 75,
      "tokens_per_second": 2800000,
      "avg_tokens_per_request": 1500,
      "avg_latency_seconds": 0.095,
      "model_variant": "mistral_7b_32k_20240522",
      "snapshot_date": "2024-11-20"
    },
    {
      "model_resource_name": "anthropic/claude-3-haiku",
      "model_display_name": "claude-3-haiku",
      "model_version": "20240307",
      "publisher_name": "Anthropic",
      "model_family": "claude-3",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "global",
      "endpoint": "https://api.anthropic.com/v1/messages",
      "accelerator_type": "A10",
      "accelerators_per_replica": 1,
      "memory_gb": 40,
      "replicas": 28,
      "max_concurrency": 180,
      "ideal_concurrency": 150,
      "max_rps": 90,
      "tokens_per_second": 2600000,
      "avg_tokens_per_request": 1700,
      "avg_latency_seconds": 0.1,
      "model_variant": "claude-3-haiku_200k_20240307",
      "snapshot_date": "2025-02-01"
    },
    {
      "model_resource_name": "google/gemini-1.0-pro",
      "model_display_name": "gemini-1.0-pro",
      "model_version": "1.0",
      "publisher_name": "Google",
      "model_family": "gemini",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "global",
      "endpoint": "https://generativelanguage.googleapis.com/v1/models",
      "accelerator_type": "TPU-v4",
      "accelerators_per_replica": 2,
      "memory_gb": 60,
      "replicas": 16,
      "max_concurrency": 90,
      "ideal_concurrency": 70,
      "max_rps": 48,
      "tokens_per_second": 1500000,
      "avg_tokens_per_request": 2100,
      "avg_latency_seconds": 0.17,
      "model_variant": "gemini-1.0-pro_32k_20231206",
      "snapshot_date": "2024-12-01"
    },
    {
      "model_resource_name": "cohere/command",
      "model_display_name": "command",
      "model_version": "2024-03",
      "publisher_name": "Cohere",
      "model_family": "command",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "global",
      "endpoint": "https://api.cohere.ai/v1/chat",
      "accelerator_type": "A100",
      "accelerators_per_replica": 2,
      "memory_gb": 60,
      "replicas": 12,
      "max_concurrency": 70,
      "ideal_concurrency": 55,
      "max_rps": 40,
      "tokens_per_second": 1100000,
      "avg_tokens_per_request": 2000,
      "avg_latency_seconds": 0.21,
      "model_variant": "command_128k_20240301",
      "snapshot_date": "2025-01-15"
    },
    {
      "model_resource_name": "ai21/jamba-1.5-large",
      "model_display_name": "jamba-1.5-large",
      "model_version": "1.5",
      "publisher_name": "AI21 Labs",
      "model_family": "jamba",
      "model_task": "text-generation",
      "is_open_source": false,
      "inference_scope": "multi-region",
      "endpoint": "https://api.ai21.com/studio/v1/chat/completions",
      "accelerator_type": "A100",
      "accelerators_per_replica": 4,
      "memory_gb": 120,
      "replicas": 10,
      "max_concurrency": 50,
      "ideal_concurrency": 40,
      "max_rps": 35,
      "tokens_per_second": 950000,
      "avg_tokens_per_request": 2400,
      "avg_latency_seconds": 0.26,
      "model_variant": "jamba-1.5-large_256k_20240815",
      "snapshot_date": "2025-01-10"
    }
  ],
  "metadata": {
  "source": "model_serving_platform",
  "extracted_at": "2026-02-13T19:30:00Z",
  "total_models": 20,
  "open_source_count": 6,
  "proprietary_count": 14,
  "data_version": "1.0",
  "model_variant_format": "modelname_contextsize_releasedate (all lowercase)",
  "field_name_changes": {
    "provider_name": "publisher_name",
    "traffic_type": "inference_scope"
  },
  "inference_scope_values": [
    "global",
    "regional",
    "multi-region"
  ],
  "data_classification": {
    "publicly_available": [
      "publisher_name",
      "provider_type",
      "model_name",
      "model_version",
      "model_task",
      "model_family",
      "endpoint",
      "is_open_source",
      "model_variant (model name, context window size, release date are publicly documented)"
    ],
    "synthesized_from_public_benchmarks": [
      "accelerator_type (common GPU/TPU types for these model sizes)",
      "gpus_per_replica (estimated from model size and typical deployments)",
      "memory_gb (estimated from model parameters and precision)",
      "tokens_per_second (derived from public benchmarks)",
      "avg_tokens_per_request (typical usage patterns)",
      "avg_latency_seconds (based on public API performance reports)"
    ],
    "synthesized_deployment_estimates": [
      "replicas (estimated production scale)",
      "max_concurrency (estimated from typical API limits)",
      "ideal_concurrency (estimated optimal throughput)",
      "max_rps (estimated rate limits)",
      "inference_scope (inferred deployment scope)",
      "platform_name (standardized value)",
      "snapshot_date (date of data compilation)"
    ]
  },
  "notes": [
    "Model identifiers, versions, context windows, and release dates are taken from official provider documentation where available.",
    "Performance metrics were synthesized from public benchmarks and API documentation; these are estimates and will vary by deployment and workload.",
    "Deployment configuration fields (replicas, concurrency, max_rps) are estimated from typical production patterns and are not authoritative for any specific deployment.",
    "inference_scope values normalized to: global, regional, multi-region (replaces traffic_type).",
    "Field names provider_name -> publisher_name and traffic_type -> inference_scope were standardized in this dataset."
  ]
  }
}